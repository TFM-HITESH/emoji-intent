{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a1aa1a-5375-4903-a54d-ae97273c6482",
   "metadata": {
    "id": "99a1aa1a-5375-4903-a54d-ae97273c6482"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tanikina/low-resource-nlp-lab/blob/main/notebooks/PEFT_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409055f3-2222-4835-80c5-5978a7ab25d0",
   "metadata": {
    "id": "409055f3-2222-4835-80c5-5978a7ab25d0"
   },
   "source": [
    "## ü§ó PEFT\n",
    "PEFT stands for Parameter-Efficient Fine-Tuning. [PEFT Library](https://github.com/huggingface/peft) supports different adaptation methods for PLMs by fine-tuning only a small number of parameters instead of updating all the model's parameters which decreases computational and storage costs. E.g., prompt tuning, LoRA and IA3 are suppoted by `PEFT` and it is also integrated with `Transformers` and `Accelerate` libraries to support distributed training and inference for very big models.\n",
    "\n",
    "You can find some addditional tutorials and examples using PEFT here: [https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8d023-0317-4e9a-b53b-4e1848a7d5e0",
   "metadata": {
    "id": "bae8d023-0317-4e9a-b53b-4e1848a7d5e0"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install datasets\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ae30e-82f6-4f76-98f7-716bf4064052",
   "metadata": {
    "id": "ab7ae30e-82f6-4f76-98f7-716bf4064052"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f79c3d-f407-4942-9f9c-0996768639d5",
   "metadata": {
    "id": "73f79c3d-f407-4942-9f9c-0996768639d5"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Defining the task and hyperparameters\n",
    "task = \"intent_classification\"\n",
    "model_name = \"xlm-roberta-base\"\n",
    "batch_size = 16\n",
    "num_epochs = 20\n",
    "encode_prev_turn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8766dd-65fb-42e2-93cc-cdf469e43874",
   "metadata": {
    "id": "0b8766dd-65fb-42e2-93cc-cdf469e43874"
   },
   "outputs": [],
   "source": [
    "# Combine the speaker information with the input text\n",
    "def add_speaker(example):\n",
    "    example[\"text\"] = example[\"speaker\"] + \" - \" + example[\"text\"]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814068a-60d3-4c06-9464-d83c384257fd",
   "metadata": {
    "id": "d814068a-60d3-4c06-9464-d83c384257fd",
    "outputId": "53717b55-850c-4c22-cd85-9fe77794f4a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': 2, 'input_ids': [0, 345, 27816, 20, 345, 27816, 1256, 23752, 38953, 13, 4, 1439, 108879, 198, 404, 186, 169846, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['UAV - UAV hat Softwareprobleme, wir versuchen es zu beheben.']\n",
      "{'labels': 2, 'input_ids': [0, 20602, 20, 823, 4, 493, 43254, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['TL - Ja, verstanden.']\n",
      "{'labels': 1, 'input_ids': [0, 20602, 20, 40787, 111697, 4, 2964, 4077, 599, 38250, 87523, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['TL - Bitte melden, wenn wieder einsatzbereit.']\n"
     ]
    }
   ],
   "source": [
    "# Defining the tokenizer and pre-processing the data\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data = datasets.load_dataset(\"DFKI/radr_intents\")\n",
    "# Preparing the training data\n",
    "train_task_dataset = train_task_dataset = data[\"train\"] # datasets.Dataset.from_csv(\"radr_intents/train.csv\")\n",
    "train_task_dataset = train_task_dataset.map(add_speaker)\n",
    "train_task_dataset = train_task_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "train_task_dataset = train_task_dataset.rename_column(\"label\",\"labels\")\n",
    "train_task_dataset = train_task_dataset.remove_columns(['id', 'speaker', 'text'])\n",
    "\n",
    "# Preparing the validation data\n",
    "dev_task_dataset = dev_task_dataset = data[\"validation\"] # datasets.Dataset.from_csv(\"radr_intents/dev.csv\")\n",
    "dev_task_dataset = dev_task_dataset.map(add_speaker)\n",
    "dev_task_dataset = dev_task_dataset.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "dev_task_dataset = dev_task_dataset.rename_column(\"label\",\"labels\")\n",
    "dev_task_dataset = dev_task_dataset.remove_columns(['id', 'speaker', 'text'])\n",
    "\n",
    "# Printing some examples\n",
    "for sample_i, sample in enumerate(dev_task_dataset):\n",
    "    if sample_i > 2:\n",
    "        break\n",
    "    print(sample)\n",
    "    print(tokenizer.batch_decode([sample[\"input_ids\"][:30]], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a2d0e0-5ec8-4955-8990-28dbe5dd0814",
   "metadata": {
    "id": "47a2d0e0-5ec8-4955-8990-28dbe5dd0814"
   },
   "source": [
    "### ‚öôÔ∏è Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef6109-448a-44fb-ba94-5e2845e53ed8",
   "metadata": {
    "id": "0cef6109-448a-44fb-ba94-5e2845e53ed8"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    PromptEncoderConfig,\n",
    "    LoraConfig,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "task = \"radr_intents\"\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa0a7d-ea0c-45df-997e-be1425906517",
   "metadata": {
    "id": "bdaa0a7d-ea0c-45df-997e-be1425906517"
   },
   "source": [
    "Here we can choose between `PromptEncoderConfig` for P-tuning introduced in [GPT Understands, Too (Liu et al., 2021)](https://www.semanticscholar.org/paper/GPT-Understands%2C-Too-Liu-Zheng/bc37c6bdb8f39929a58b30464f72d6aa46cddc17) and `LoraConfig` for [LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)](https://www.semanticscholar.org/paper/LoRA%3A-Low-Rank-Adaptation-of-Large-Language-Models-Hu-Shen/a8ca46b171467ceb2d7652fbfb67fe701ad86092)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d7c40-2811-4c9e-8e13-d9527068d362",
   "metadata": {
    "id": "bd0d7c40-2811-4c9e-8e13-d9527068d362"
   },
   "outputs": [],
   "source": [
    "peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=30, encoder_hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b052c-e38e-4107-8981-bac78cba8988",
   "metadata": {
    "id": "191b052c-e38e-4107-8981-bac78cba8988"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            bias=\"none\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            target_modules=[\"key\", \"query\", \"value\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92808b06-5c64-4144-867d-2d54dcc73f98",
   "metadata": {
    "id": "92808b06-5c64-4144-867d-2d54dcc73f98",
    "outputId": "d334808e-1c27-4895-e046-1777d873cc9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 833,800 || all params: 278,883,600 || trainable%: 0.2989777814113128\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8, return_dict=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbfca0-e0e5-4ca9-95ca-c13a9f128072",
   "metadata": {
    "id": "cddbfca0-e0e5-4ca9-95ca-c13a9f128072"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364c5e1-7f04-4402-afee-1d31d12db507",
   "metadata": {
    "id": "0364c5e1-7f04-4402-afee-1d31d12db507"
   },
   "source": [
    "### üöÄ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d5222-4866-4c2d-8610-787a4b4437b0",
   "metadata": {
    "id": "923d5222-4866-4c2d-8610-787a4b4437b0"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"roberta-base-peft\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    full_determinism=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_task_dataset,\n",
    "    eval_dataset=dev_task_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6debcb-db00-43e6-8bec-72370a00793c",
   "metadata": {
    "id": "bc6debcb-db00-43e6-8bec-72370a00793c",
    "outputId": "460617b4-2937-41dc-f75c-ffac4c62e1c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3280' max='3280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3280/3280 03:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.769225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.761213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.809234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.780500</td>\n",
       "      <td>1.827829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.780500</td>\n",
       "      <td>1.549241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.780500</td>\n",
       "      <td>1.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.523500</td>\n",
       "      <td>1.107880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.523500</td>\n",
       "      <td>1.147788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.523500</td>\n",
       "      <td>1.077515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.196300</td>\n",
       "      <td>1.099513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.196300</td>\n",
       "      <td>0.974644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.196300</td>\n",
       "      <td>1.053547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>1.014303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>0.955784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>0.929525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>0.965591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>0.952893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.954900</td>\n",
       "      <td>0.921041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>0.894567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>0.877211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3280, training_loss=1.2007528537657204, metrics={'train_runtime': 210.9168, 'train_samples_per_second': 247.491, 'train_steps_per_second': 15.551, 'total_flos': 885640102603776.0, 'train_loss': 1.2007528537657204, 'epoch': 20.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0579bb-acea-4272-93d0-d8d4f7faf9dd",
   "metadata": {
    "id": "5b0579bb-acea-4272-93d0-d8d4f7faf9dd"
   },
   "source": [
    "### ‚úÖ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc360a2d-78e5-40c0-8042-abd1e0ceab6b",
   "metadata": {
    "id": "bc360a2d-78e5-40c0-8042-abd1e0ceab6b",
    "outputId": "00c2bd56-eba3-4654-e254-3f95d621afeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5557, -1.8112, -2.0377, -0.0248,  7.3617,  0.9375, -0.8120, -2.7155],\n",
      "        [ 0.2110,  1.4479,  3.4419,  3.2218, -5.0600, -3.8530, -0.5526,  0.1037],\n",
      "        [-0.8891,  1.6071,  1.1785,  5.2296, -3.2375, -3.1483, -0.1494, -1.5106]],\n",
      "       device='cuda:0')\n",
      "UAV - UAV f√ºr Teamleader >>> call\n",
      "UGV 2 - Wir haben einen Kanister im Ergeschoss gefunden >>> info_provide\n",
      "TL - Was ist deine aktuelle Position? >>> info_request\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'disconfirm', 1: 'order', 2: 'info_provide', 3: 'info_request', 4: 'call', 5: 'call_response', 6: 'other', 7: 'confirm'}\n",
    "texts = [\"Caller - I would like to order a pizza\", \"Agent - What toppings would you like?\", \"Caller - I want pepperoni and mushrooms\", \"Agent - Would you like to add a drink to your order?\"]\n",
    "inputs = tokenizer(texts, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "    print(outputs)\n",
    "    labels = [id2label[label] for label in torch.argmax(outputs, dim=-1).tolist()]\n",
    "    for text, label in zip(texts, labels):\n",
    "        print(text, \">>>\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9761db",
   "metadata": {},
   "source": [
    "# PEFT Fine-tuning for Agent 1 (Keyword Extraction)\n",
    "This notebook adds cells to fine-tune a causal LLM (e.g., `gpt2`, `facebook/opt-...`, or `meta-...`) using **PEFT (LoRA)** to perform **Agent 1: Keyword Extraction**.\n",
    "\n",
    "**Assumptions & dataset:**\n",
    "- You have a CSV file with two columns: `conversations` and `extracted_keywords`.  \n",
    "- Each cell in `conversations` is a JSON string containing the conversation thread (as in the project spec).  \n",
    "- Each cell in `extracted_keywords` is a JSON string representing the target keywords array (the expected output from Agent 1).  \n",
    "- The model will be trained to output a valid JSON string that matches the target schema.\n",
    "\n",
    "The following cells:\n",
    "1. Install required packages.\n",
    "2. Load and preview the CSV.\n",
    "3. Prepare prompt/response pairs.\n",
    "4. Tokenize and create HuggingFace Dataset.\n",
    "5. Configure PEFT (LoRA) and Trainer.\n",
    "6. Train and save the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment and run if needed)\n",
    "# !pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece safetensors\n",
    "print('Install packages if running in a fresh environment: transformers, datasets, accelerate, peft, bitsandbytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "data_path = Path('/mnt/data/agent1_dataset.csv')  # <-- Replace with your CSV path\n",
    "assert data_path.exists(), f\"Dataset CSV not found at {data_path}. Please upload it.\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print('Total rows:', len(df))\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3748d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_prompt(convo_json_str: str) -> str:\n",
    "    # The prompt instructs the model to extract keywords and return ONLY a valid JSON array string.\n",
    "    prompt = (\"You are Agent 1: a keyword extractor that reads a conversation JSON (string) and returns \\\\\"\n",
    "              \"a JSON array of objects with fields: id, author, keywords. The output must be a valid JSON string.\\\\n\\\\n\")\n",
    "    prompt += \"Conversation JSON:\\n\" + convo_json_str + \"\\n\\nExtracted keywords JSON:\"\n",
    "    return prompt\n",
    "\n",
    "def make_target(keywords_json_str: str) -> str:\n",
    "    # target should be the exact JSON string representing the keywords array\n",
    "    # Ensure it's compact (no newlines) to simplify tokenization\n",
    "    obj = json.loads(keywords_json_str) if isinstance(keywords_json_str, str) else keywords_json_str\n",
    "    return json.dumps(obj, ensure_ascii=False)\n",
    "\n",
    "# Build examples\n",
    "examples = []\n",
    "for idx, row in df.iterrows():\n",
    "    convo = row['conversations']\n",
    "    keywords = row['extracted_keywords']\n",
    "    try:\n",
    "        prompt = make_prompt(convo)\n",
    "        target = make_target(keywords)\n",
    "        examples.append({'prompt': prompt, 'target': target})\n",
    "    except Exception as e:\n",
    "        # skip malformed rows but log\n",
    "        print(f\"Skipping row {idx} due to error: {e}\")\n",
    "\n",
    "print('Prepared examples:', len(examples))\n",
    "# show one example\n",
    "import textwrap\n",
    "print('PROMPT SAMPLE:\\n', textwrap.shorten(examples[0]['prompt'], width=400))\n",
    "print('\\nTARGET SAMPLE:\\n', examples[0]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = 'gpt2'  # change to a larger, compatible model as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Create HF dataset\n",
    "ds = Dataset.from_list(examples)\n",
    "\n",
    "# Define a simple tokenization mapping: inputs are prompt, labels are prompt + target with labels masking prompt tokens\n",
    "max_length = 512\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    prompt = example['prompt']\n",
    "    target = example['target']\n",
    "    input_text = prompt + ' ' + target\n",
    "    tokenized = tokenizer(input_text, truncation=True, max_length=max_length, padding='max_length')\n",
    "    # create labels: mask prompt tokens with -100 so loss only computed on target tokens\n",
    "    prompt_tokens = tokenizer(prompt, truncation=True, max_length=max_length, padding='max_length')\n",
    "    # find prompt length (number of non-pad tokens)\n",
    "    prompt_len = sum(1 for t in prompt_tokens['input_ids'] if t != tokenizer.pad_token_id)\n",
    "    labels = tokenized['input_ids'].copy()\n",
    "    for i in range(min(prompt_len, len(labels))):\n",
    "        labels[i] = -100\n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_fn, remove_columns=ds.column_names, batched=False)\n",
    "print(tokenized_ds[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c21e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)\n",
    "# Resize token embeddings if we added pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['c_attn', 'q_proj', 'v_proj'] if 'gpt' in model_name_or_path else None,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./agent1_fineturn',\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b40484",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "# Save\n",
    "trainer.save_model('./agent1_finetuned_model')\n",
    "tokenizer.save_pretrained('./agent1_finetuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference using the fine-tuned model\n",
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', model='./agent1_finetuned_model', tokenizer=tokenizer, device=0 if False else -1)\n",
    "sample_prompt = examples[0]['prompt']\n",
    "print('PROMPT:\\n', sample_prompt)\n",
    "out = pipe(sample_prompt, max_length=512, do_sample=False, num_return_sequences=1)\n",
    "print('\\nGENERATED:\\n', out[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
